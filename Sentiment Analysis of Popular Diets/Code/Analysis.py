# -*- coding: utf-8 -*-
"""SMM_VADER.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16gfYxSW37NDpSHwXOfI3rv9wD3tZgbjG
"""

import pandas as pd
from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer
import string
import nltk
from nltk.corpus import stopwords
from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation

nltk.download('stopwords')
stop_words = set(stopwords.words('english')).union({'diet', 'eat', 'like', 'food', 'people', 'eating'})

df = pd.read_csv('reddit_data_final.csv')
df = df.loc[:, ['Date', 'Text', 'Keyword']]

sid_obj = SentimentIntensityAnalyzer()

def preprocess_data(df):
    def clean_text(text):
        # Convert to lowercase, remove punctuation, and tokenize
        text = text.lower().translate(str.maketrans('', '', string.punctuation))
        tokens = text.split()
        # Remove stopwords and keep only alphabetic tokens
        tokens = [word for word in tokens if word not in stop_words and word.isalpha()]
        # Join tokens back into a string
        return ' '.join(tokens)

    # Apply text cleaning to the 'Text' column
    df["Text"] = df["Text"].apply(clean_text)
    # Convert the 'Date' column to datetime format
    df["Date"] = pd.to_datetime(df["Date"])
    return df

def sentiment(df):
    sentiment_scores = []
    sentiment_categories = []

    for text in df['Text']:
        scores = sid_obj.polarity_scores(text)
        sentiment_score = scores['compound']
        sentiment_scores.append(sentiment_score)

        # Classify the sentiment
        if sentiment_score > 0.05:
            sentiment_categories.append('positive')
        elif sentiment_score < -0.05:
            sentiment_categories.append('negative')
        else:
            sentiment_categories.append('neutral')

    df['sentiment_score'] = sentiment_scores
    df['sentiment'] = sentiment_categories



df = preprocess_data(df)
sentiment(df)

df

# Count the number of positive, negative, and neutral sentiments for each keyword
sentiment_counts = df.groupby('Keyword')['sentiment'].value_counts().unstack().fillna(0).astype(int)

# Add a total count column for each keyword
sentiment_counts['Total'] = sentiment_counts.sum(axis=1)

# Print the sentiment counts with the total for each keyword
print(sentiment_counts)

import matplotlib.pyplot as plt
import seaborn as sns
# Creating a different type of visualization: a grouped bar chart
plt.figure(figsize=(10, 6))
sentiment_counts.plot(kind='bar', color=['#2E8B57', '#66CDAA', '#6B8E23'])

# Add labels and title
plt.title('Sentiment Distribution Across Keywords', fontsize=14)
plt.xlabel('Keyword', fontsize=12)
plt.ylabel('Count', fontsize=12)
plt.xticks(rotation=45)
plt.legend(title='Sentiment', fontsize=10)
plt.tight_layout()

# Show the plot
plt.show()

def apply_lda(df, num_topics=5, num_words=7):
    for keyword in df['Keyword'].unique():
        print(f"\nTopics for {keyword} discussions:")
        keyword_texts = df[df['Keyword'] == keyword]['Text'].apply(clean_text_for_lda)

        # Vectorize the text data with bi-grams
        vectorizer = CountVectorizer(max_df=0.9, min_df=2, stop_words='english', ngram_range=(1, 2))
        dtm = vectorizer.fit_transform(keyword_texts)

        # Apply LDA
        lda = LatentDirichletAllocation(n_components=num_topics, random_state=0)
        lda.fit(dtm)

        # Display the topics
        words = vectorizer.get_feature_names_out()
        for i, topic in enumerate(lda.components_):
            print(f"Topic {i + 1}: ", [words[j] for j in topic.argsort()[-num_words:]])

apply_lda(df)