# -*- coding: utf-8 -*-
"""SMM_Paper3.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TWKAtqefXgW0BtW8ZEwNor4wSUcNtcwd
"""

import pandas as pd
import re
from empath import Empath

!pip install empath

# Load the dataset
file_path = '/content/airlines_reviews.csv'
data = pd.read_csv(file_path)

# Display the first few rows of the dataset to understand its structure
data.head()

# Step 1: Cleaning the text data for Empath analysis
def clean_text(text):
    """
    Cleans text by removing special characters, extra spaces, and converting to lowercase.
    """
    if isinstance(text, str):
        text = re.sub(r'\s+', ' ', text)  # Remove extra spaces
        text = re.sub(r'[^\w\s]', '', text)  # Remove special characters
        return text.lower().strip()
    return ""

# Apply cleaning to the Reviews column
data['Cleaned_Reviews'] = data['Reviews'].apply(clean_text)

# Step 2: Check the distribution of reviews after cleaning
cleaned_sample = data[['Reviews', 'Cleaned_Reviews']].head()

# Display the cleaned samples
cleaned_sample

lexicon = Empath()

def get_selected_empath_scores(text):
    """
    Extracts selected sentiment scores for a given text using Empath.
    Returns a dictionary with only necessary features.
    """
    if text:
        scores = lexicon.analyze(text, categories=None, normalize=True)
        # Select only the features relevant for supervised learning
        selected_features = ['positive_emotion', 'negative_emotion', 'contentment', 'trust', 'anger', 'fear', 'joy']
        return {feature: scores.get(feature, 0) for feature in selected_features}
    return {}

data['Selected_Empath_Scores'] = data['Cleaned_Reviews'].apply(get_selected_empath_scores)

# Step 4: Normalize the selected Empath scores into separate columns
empath_selected_df = pd.json_normalize(data['Selected_Empath_Scores'])

# Combine selected Empath scores with the original data
data_with_selected_empath = pd.concat([data, empath_selected_df], axis=1)

data_with_selected_empath

final_columns = [
    'Seat Comfort', 'Staff Service', 'Food & Beverages',
    'Inflight Entertainment', 'Value For Money', 'Overall Rating',
    'positive_emotion', 'negative_emotion', 'contentment', 'trust', 'anger', 'fear', 'joy',
    'Recommended'
]
data_filtered = data_with_selected_empath[final_columns]

data_filtered

"""SUPERVISED LEARNING - RANDOM FOREST

"""

data_filtered['Recommended'] = data_filtered['Recommended'].apply(lambda x: 1 if x == 'yes' else 0)

X = data_filtered.drop(columns=['Recommended'])
y = data_filtered['Recommended']

from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

param_grid = {
    'n_estimators': [50, 100, 200],  # Number of trees
    'max_depth': [None, 10, 20],     # Maximum depth of each tree
    'min_samples_split': [2, 5, 10], # Minimum samples required to split a node
    'min_samples_leaf': [1, 2, 4],   # Minimum samples required at each leaf node
}

# Perform Grid Search with Cross Validation
grid_search = GridSearchCV(
    estimator=RandomForestClassifier(random_state=42, class_weight='balanced'),
    param_grid=param_grid,
    scoring='f1',  # Optimize for F1-score
    cv=3,          # 3-fold cross-validation
    verbose=1,     # Print progress
    n_jobs=-1      # Use all available CPU cores
)

grid_search.fit(X_train, y_train)

best_params_rf = grid_search.best_params_
best_rf_model = grid_search.best_estimator_
y_pred_best_rf = best_rf_model.predict(X_test)

accuracy_best_rf = accuracy_score(y_test, y_pred_best_rf)
precision_best_rf = precision_score(y_test, y_pred_best_rf)
recall_best_rf = recall_score(y_test, y_pred_best_rf)
f1_best_rf = f1_score(y_test, y_pred_best_rf)
classification_rep_best_rf = classification_report(y_test, y_pred_best_rf)

print("Best Hyperparameters:", best_params_rf)
print("Accuracy:", accuracy_best_rf)
print("Precision:", precision_best_rf)
print("Recall:", recall_best_rf)
print("F1 Score:", f1_best_rf)
print("Classification Report:\n", classification_rep_best_rf)

feature_importances = best_rf_model.feature_importances_

# Create a DataFrame for visualization
importance_df = pd.DataFrame({
    'Feature': X.columns,
    'Importance': feature_importances
}).sort_values(by='Importance', ascending=False)

# Display the top features
print(importance_df)

# Optional: Visualize the feature importances
plt.figure(figsize=(10, 6))
plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')
plt.gca().invert_yaxis()  # Invert axis for better visualization
plt.xlabel('Feature Importance')
plt.ylabel('Feature')
plt.title('Feature Importance in Random Forest')
plt.show()

"""UNSUPERVISED LEARNING - TOPIC MODELING"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.decomposition import LatentDirichletAllocation
import nltk
from nltk.corpus import stopwords

nltk.download('stopwords')
stop_words = set(stopwords.words('english'))

def preprocess_reviews(text):
    """
    Preprocess text by tokenizing, removing stop words, and keeping only words with length > 2.
    """
    if isinstance(text, str):
        tokens = [word for word in text.lower().split() if word not in stop_words and len(word) > 2]
        return " ".join(tokens)
    return ""

# Apply preprocessing to the Cleaned_Reviews column
data_with_selected_empath['Processed_Reviews'] = data_with_selected_empath['Cleaned_Reviews'].apply(preprocess_reviews)

vectorizer = CountVectorizer(max_features=1000)  # Limit to top 1000 words for simplicity
dtm = vectorizer.fit_transform(data_with_selected_empath['Processed_Reviews'])

lda_model = LatentDirichletAllocation(n_components=5, random_state=42)  # 5 topics as an example
lda_model.fit(dtm)

def display_topics(model, feature_names, no_top_words):
    topics = {}
    for topic_idx, topic in enumerate(model.components_):
        top_features = [feature_names[i] for i in topic.argsort()[:-no_top_words - 1:-1]]
        topics[f"Topic {topic_idx+1}"] = top_features
    return topics

no_top_words = 10
topics = display_topics(lda_model, vectorizer.get_feature_names_out(), no_top_words)

for topic, words in topics.items():
    print(f"{topic}: {', '.join(words)}")

import matplotlib.pyplot as plt
from wordcloud import WordCloud
import numpy as np
import math

importance_values = {topic: np.random.random(len(words)) for topic, words in topics.items()}

# Function to plot bar charts in a square layout
def plot_bar_charts_square(topics, importance_values):
    n_topics = len(topics)
    rows = math.ceil(n_topics / 2)  # Arrange in 2 columns
    cols = 2

    fig, axes = plt.subplots(rows, cols, figsize=(10, 8), sharey=True)  # Square-like frame
    axes = axes.flatten()  # Flatten for easier iteration

    for idx, (topic, words) in enumerate(topics.items()):
        axes[idx].bar(words, importance_values[topic], color="skyblue")
        axes[idx].set_title(topic, fontsize=12)
        axes[idx].set_xticks(range(len(words)))
        axes[idx].set_xticklabels(words, rotation=45, fontsize=8)
        axes[idx].set_ylabel("Importance" if idx % cols == 0 else "")

    # Turn off any empty axes
    for ax in axes[len(topics):]:
        ax.axis('off')

    # Add a global title
    fig.suptitle("Figure 2: Word Importance Across Topics", fontsize=14, y=1.02)

    plt.tight_layout()
    plt.show()

# Call the function
plot_bar_charts_square(topics, importance_values)

import matplotlib.pyplot as plt
from wordcloud import WordCloud
import math

def plot_word_clouds(topics, cols=2):
    """
    Generate word clouds for each topic, arranged in a grid layout.

    Args:
        topics (dict): Dictionary of topics and their associated words.
        cols (int): Number of columns in the grid layout.
    """
    n_topics = len(topics)
    rows = math.ceil(n_topics / cols)  # Calculate the number of rows needed

    fig, axes = plt.subplots(rows, cols, figsize=(cols * 6, rows * 4))
    axes = axes.flatten()  # Flatten the axes array for easy iteration

    for idx, (topic, words) in enumerate(topics.items()):
        wordcloud = WordCloud(background_color='white', width=800, height=400).generate(" ".join(words))
        axes[idx].imshow(wordcloud, interpolation='bilinear')
        axes[idx].axis('off')
        axes[idx].set_title(f"{topic}", fontsize=14)

    # Turn off any unused subplots
    for ax in axes[len(topics):]:
        ax.axis('off')

    plt.tight_layout()
    plt.show()

# Generate word clouds for topics in a grid layout
plot_word_clouds(topics, cols=2)